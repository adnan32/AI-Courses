{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input\n",
    "from keras.optimizers import Adam\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Synthetic-Persona-Chat_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vald = pd.read_csv('Synthetic-Persona-Chat_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= pd.read_csv('Synthetic-Persona-Chat_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user 1 personas</th>\n",
       "      <th>user 2 personas</th>\n",
       "      <th>Best Generated Conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am 32.\\nI do not want a job.\\nI play video g...</td>\n",
       "      <td>My favorite drink is iced coffee.\\nI have a bl...</td>\n",
       "      <td>User 1: Hi! I'm [user 1's name].\\nUser 2: Hi [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am 32.\\nI play video games all day.\\nI still...</td>\n",
       "      <td>I have a ford f150.\\nI like ford cars.\\nMy tru...</td>\n",
       "      <td>User 1: Hey, how's it going?\\nUser 2: Good, I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am 32.\\nI play video games all day.\\nI still...</td>\n",
       "      <td>I can recite the movie young frankenstein word...</td>\n",
       "      <td>User 1: Hi, my name is John. What's your name?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I write.\\nI work at mcdonald s.\\nI watch youtu...</td>\n",
       "      <td>I want to move.\\nI don t like feeling controll...</td>\n",
       "      <td>User 1: Hi!\\nUser 2: Hey!\\nUser 1: What's up?\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am bald.\\nI like to swim.\\nMy favorite drink...</td>\n",
       "      <td>My favorite store is american eagle.\\nI enjoy ...</td>\n",
       "      <td>User 1: Hello!\\nUser 2: Hi!\\nUser 1: What do y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8933</th>\n",
       "      <td>My parents used to work in politics , until th...</td>\n",
       "      <td>I am in a very intimate and loving relationshi...</td>\n",
       "      <td>User 1: Hey, what's up?\\nUser 2: Not much, jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8934</th>\n",
       "      <td>My parents used to work in politics , until th...</td>\n",
       "      <td>My mind is set on things above.\\nI hate evil.\\...</td>\n",
       "      <td>User 1: Hey!\\nUser 2: Hello!\\nUser 1: What do ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8935</th>\n",
       "      <td>I went on welfare last month , which makes me ...</td>\n",
       "      <td>I got married last year.\\nI am a hair stylist....</td>\n",
       "      <td>User 1: Hey, I'm [name].\\nUser 2: Hi, I'm [nam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8936</th>\n",
       "      <td>I went on welfare last month , which makes me ...</td>\n",
       "      <td>I wish i made more money.\\nI have a strange ob...</td>\n",
       "      <td>User 1: Hi, I'm [user 1].\\n\\nUser 2: Hi, I'm [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8937</th>\n",
       "      <td>I went on welfare last month , which makes me ...</td>\n",
       "      <td>I enjoy reading history books.\\nI love to danc...</td>\n",
       "      <td>User 1: Hello!\\nUser 2: Hello! How are you doi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8938 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        user 1 personas  \\\n",
       "0     I am 32.\\nI do not want a job.\\nI play video g...   \n",
       "1     I am 32.\\nI play video games all day.\\nI still...   \n",
       "2     I am 32.\\nI play video games all day.\\nI still...   \n",
       "3     I write.\\nI work at mcdonald s.\\nI watch youtu...   \n",
       "4     I am bald.\\nI like to swim.\\nMy favorite drink...   \n",
       "...                                                 ...   \n",
       "8933  My parents used to work in politics , until th...   \n",
       "8934  My parents used to work in politics , until th...   \n",
       "8935  I went on welfare last month , which makes me ...   \n",
       "8936  I went on welfare last month , which makes me ...   \n",
       "8937  I went on welfare last month , which makes me ...   \n",
       "\n",
       "                                        user 2 personas  \\\n",
       "0     My favorite drink is iced coffee.\\nI have a bl...   \n",
       "1     I have a ford f150.\\nI like ford cars.\\nMy tru...   \n",
       "2     I can recite the movie young frankenstein word...   \n",
       "3     I want to move.\\nI don t like feeling controll...   \n",
       "4     My favorite store is american eagle.\\nI enjoy ...   \n",
       "...                                                 ...   \n",
       "8933  I am in a very intimate and loving relationshi...   \n",
       "8934  My mind is set on things above.\\nI hate evil.\\...   \n",
       "8935  I got married last year.\\nI am a hair stylist....   \n",
       "8936  I wish i made more money.\\nI have a strange ob...   \n",
       "8937  I enjoy reading history books.\\nI love to danc...   \n",
       "\n",
       "                            Best Generated Conversation  \n",
       "0     User 1: Hi! I'm [user 1's name].\\nUser 2: Hi [...  \n",
       "1     User 1: Hey, how's it going?\\nUser 2: Good, I'...  \n",
       "2     User 1: Hi, my name is John. What's your name?...  \n",
       "3     User 1: Hi!\\nUser 2: Hey!\\nUser 1: What's up?\\...  \n",
       "4     User 1: Hello!\\nUser 2: Hi!\\nUser 1: What do y...  \n",
       "...                                                 ...  \n",
       "8933  User 1: Hey, what's up?\\nUser 2: Not much, jus...  \n",
       "8934  User 1: Hey!\\nUser 2: Hello!\\nUser 1: What do ...  \n",
       "8935  User 1: Hey, I'm [name].\\nUser 2: Hi, I'm [nam...  \n",
       "8936  User 1: Hi, I'm [user 1].\\n\\nUser 2: Hi, I'm [...  \n",
       "8937  User 1: Hello!\\nUser 2: Hello! How are you doi...  \n",
       "\n",
       "[8938 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user 1 personas</th>\n",
       "      <th>user 2 personas</th>\n",
       "      <th>Best Generated Conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love to bake cookies.\\nI have a dogs.\\nThe c...</td>\n",
       "      <td>I am a boy.\\nI can move objects with my mind.\\...</td>\n",
       "      <td>User 1: Hi!\\nUser 2: Hello!\\nUser 1: What is y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am an animal activist.\\nThe holidays make me...</td>\n",
       "      <td>I feel old.\\nI am currently in a juvenile dete...</td>\n",
       "      <td>User 1: Hi, how are you?\\nUser 2: I'm doing ok...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cheeseburgers are my favorite food.\\nI like wa...</td>\n",
       "      <td>I feel old.\\nI am currently in a juvenile dete...</td>\n",
       "      <td>User 1: Hello, my name is (name).\\nUser 2: Hel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I plan to go to business school next year.\\nMy...</td>\n",
       "      <td>I am a male.\\nI own a house in florida.\\nI hav...</td>\n",
       "      <td>User 1: I'm looking forward to going to busine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I can drive a tractor.\\nMy favorite color is r...</td>\n",
       "      <td>I am a male.\\nI have a children and a dogs.\\nI...</td>\n",
       "      <td>User 1: I'm so glad we're both going to the co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>I attend church every week.\\nI work as a schoo...</td>\n",
       "      <td>I live in california but the recording artist ...</td>\n",
       "      <td>User 1: Hi, I'm a school teacher and I attend ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>I am in an open polyamorous relationship.\\nI a...</td>\n",
       "      <td>I live in california but the recording artist ...</td>\n",
       "      <td>User 1: Hi, I'm [user 1 name].\\nUser 2: Hi, I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>I do not pick up my toys.\\nI am four.\\nI have ...</td>\n",
       "      <td>I live in california but the recording artist ...</td>\n",
       "      <td>User 1: Hi!\\nUser 2: Hi! How are you doing tod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>I am an omnivore.\\nI just bought my first home...</td>\n",
       "      <td>I have an internet addiction and spend a lot o...</td>\n",
       "      <td>User 1: Hello! What are you interested in?\\nUs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>I am overweight and unhappy.\\nI work at a nurs...</td>\n",
       "      <td>I have an internet addiction and spend a lot o...</td>\n",
       "      <td>User 1: Hi there!\\nUser 2: Hello!\\nUser 1: Wha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       user 1 personas  \\\n",
       "0    I love to bake cookies.\\nI have a dogs.\\nThe c...   \n",
       "1    I am an animal activist.\\nThe holidays make me...   \n",
       "2    Cheeseburgers are my favorite food.\\nI like wa...   \n",
       "3    I plan to go to business school next year.\\nMy...   \n",
       "4    I can drive a tractor.\\nMy favorite color is r...   \n",
       "..                                                 ...   \n",
       "995  I attend church every week.\\nI work as a schoo...   \n",
       "996  I am in an open polyamorous relationship.\\nI a...   \n",
       "997  I do not pick up my toys.\\nI am four.\\nI have ...   \n",
       "998  I am an omnivore.\\nI just bought my first home...   \n",
       "999  I am overweight and unhappy.\\nI work at a nurs...   \n",
       "\n",
       "                                       user 2 personas  \\\n",
       "0    I am a boy.\\nI can move objects with my mind.\\...   \n",
       "1    I feel old.\\nI am currently in a juvenile dete...   \n",
       "2    I feel old.\\nI am currently in a juvenile dete...   \n",
       "3    I am a male.\\nI own a house in florida.\\nI hav...   \n",
       "4    I am a male.\\nI have a children and a dogs.\\nI...   \n",
       "..                                                 ...   \n",
       "995  I live in california but the recording artist ...   \n",
       "996  I live in california but the recording artist ...   \n",
       "997  I live in california but the recording artist ...   \n",
       "998  I have an internet addiction and spend a lot o...   \n",
       "999  I have an internet addiction and spend a lot o...   \n",
       "\n",
       "                           Best Generated Conversation  \n",
       "0    User 1: Hi!\\nUser 2: Hello!\\nUser 1: What is y...  \n",
       "1    User 1: Hi, how are you?\\nUser 2: I'm doing ok...  \n",
       "2    User 1: Hello, my name is (name).\\nUser 2: Hel...  \n",
       "3    User 1: I'm looking forward to going to busine...  \n",
       "4    User 1: I'm so glad we're both going to the co...  \n",
       "..                                                 ...  \n",
       "995  User 1: Hi, I'm a school teacher and I attend ...  \n",
       "996  User 1: Hi, I'm [user 1 name].\\nUser 2: Hi, I'...  \n",
       "997  User 1: Hi!\\nUser 2: Hi! How are you doing tod...  \n",
       "998  User 1: Hello! What are you interested in?\\nUs...  \n",
       "999  User 1: Hi there!\\nUser 2: Hello!\\nUser 1: Wha...  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vald"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user 1 personas</th>\n",
       "      <th>user 2 personas</th>\n",
       "      <th>Best Generated Conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I just bought a brand new house.\\nI like to da...</td>\n",
       "      <td>I love to meet new people.\\nI have a turtle na...</td>\n",
       "      <td>User 1: Hi, I'm [User 1's name]. What's your n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am an old man.\\nI have a wheelchair that is ...</td>\n",
       "      <td>I drive a ford pickup truck.\\nI am very conser...</td>\n",
       "      <td>User 1: Hi there!\\nUser 2: Hello.\\nUser 1: I'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I met my best friend in kindergarten.\\nI am of...</td>\n",
       "      <td>I have two dogs and one cat.\\nI work as a nurs...</td>\n",
       "      <td>User 1: Hi! What do you do for work?\\nUser 2: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I love having facial hair.\\nI hope to retire s...</td>\n",
       "      <td>I am happy being single and alone.\\nI only dri...</td>\n",
       "      <td>User 1: How are you doing today?\\nUser 2: I'm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I love watching movies and tv.\\nI have a husba...</td>\n",
       "      <td>I am afraid of heights.\\nI love animals and ha...</td>\n",
       "      <td>User 1: Hi there!\\nUser 2: Hi there!\\nUser 1: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>I work out every day.\\nMy favorite musician is...</td>\n",
       "      <td>I got a job working in advertising last year.\\...</td>\n",
       "      <td>User 1: Hi, my name is [user 1's name].\\nUser ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>I lift weights every weekend.\\nI am big and ta...</td>\n",
       "      <td>I am currently attending school.\\nI like music...</td>\n",
       "      <td>User 1: Hey, I'm [user 1].\\n\\nUser 2: Hi, I'm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>I am considering going to college.\\nIts a dead...</td>\n",
       "      <td>I do not eat anything sweet.\\nI do not drive b...</td>\n",
       "      <td>User 1: Hi, my name is [user 1's name].\\nUser ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>I have an exotic fish aquarium.\\nI collect vin...</td>\n",
       "      <td>I only drink water.\\nI do not want children.\\n...</td>\n",
       "      <td>User 1: Hey there!\\nUser 2: Hello!\\nUser 1: I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>I love to hang out with my friends.\\nI love pl...</td>\n",
       "      <td>I have a large collection of teddy bears.\\nI w...</td>\n",
       "      <td>User 1: Hi there!\\nUser 2: Hi there!\\nUser 1: ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>968 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       user 1 personas  \\\n",
       "0    I just bought a brand new house.\\nI like to da...   \n",
       "1    I am an old man.\\nI have a wheelchair that is ...   \n",
       "2    I met my best friend in kindergarten.\\nI am of...   \n",
       "3    I love having facial hair.\\nI hope to retire s...   \n",
       "4    I love watching movies and tv.\\nI have a husba...   \n",
       "..                                                 ...   \n",
       "963  I work out every day.\\nMy favorite musician is...   \n",
       "964  I lift weights every weekend.\\nI am big and ta...   \n",
       "965  I am considering going to college.\\nIts a dead...   \n",
       "966  I have an exotic fish aquarium.\\nI collect vin...   \n",
       "967  I love to hang out with my friends.\\nI love pl...   \n",
       "\n",
       "                                       user 2 personas  \\\n",
       "0    I love to meet new people.\\nI have a turtle na...   \n",
       "1    I drive a ford pickup truck.\\nI am very conser...   \n",
       "2    I have two dogs and one cat.\\nI work as a nurs...   \n",
       "3    I am happy being single and alone.\\nI only dri...   \n",
       "4    I am afraid of heights.\\nI love animals and ha...   \n",
       "..                                                 ...   \n",
       "963  I got a job working in advertising last year.\\...   \n",
       "964  I am currently attending school.\\nI like music...   \n",
       "965  I do not eat anything sweet.\\nI do not drive b...   \n",
       "966  I only drink water.\\nI do not want children.\\n...   \n",
       "967  I have a large collection of teddy bears.\\nI w...   \n",
       "\n",
       "                           Best Generated Conversation  \n",
       "0    User 1: Hi, I'm [User 1's name]. What's your n...  \n",
       "1    User 1: Hi there!\\nUser 2: Hello.\\nUser 1: I'm...  \n",
       "2    User 1: Hi! What do you do for work?\\nUser 2: ...  \n",
       "3    User 1: How are you doing today?\\nUser 2: I'm ...  \n",
       "4    User 1: Hi there!\\nUser 2: Hi there!\\nUser 1: ...  \n",
       "..                                                 ...  \n",
       "963  User 1: Hi, my name is [user 1's name].\\nUser ...  \n",
       "964  User 1: Hey, I'm [user 1].\\n\\nUser 2: Hi, I'm ...  \n",
       "965  User 1: Hi, my name is [user 1's name].\\nUser ...  \n",
       "966  User 1: Hey there!\\nUser 2: Hello!\\nUser 1: I'...  \n",
       "967  User 1: Hi there!\\nUser 2: Hi there!\\nUser 1: ...  \n",
       "\n",
       "[968 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and collecting conversations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> Hi! I'm [user 1's name]. <EOS>\n",
      "<SOS> Hi [user 1's name], I'm [user 2's name]. <EOS>\n",
      "<SOS> What do you do for fun? <EOS>\n",
      "<SOS> I like to play video games, go to the beach, and read. <EOS>\n",
      "<SOS> I like to play video games too! I'm not much of a reader, though. <EOS>\n",
      "<SOS> What video games do you like to play? <EOS>\n",
      "<SOS> I like to play a lot of different games, but I'm really into competitive online games right now. <EOS>\n",
      "<SOS> I'm not really into competitive games, I like to play more relaxing games. <EOS>\n",
      "<SOS> That's cool. What kind of relaxing games do you like to play? <EOS>\n",
      "<SOS> I like to play puzzle games, simulation games, and story-based games. <EOS>\n",
      "<SOS> I've never been much of a puzzle game person, but I do like simulation games and story-based games. <EOS>\n",
      "<SOS> Nice! What's your favorite simulation game? <EOS>\n",
      "<SOS> I like Stardew Valley a lot. It's a farming game, but it's also really relaxing and fun. <EOS>\n",
      "<SOS> I've heard good things about that game. I might have to check it out. <EOS>\n",
      "<SOS> You should! It's a lot of fun. <EOS>\n",
      "<SOS> Well, I'm glad we met. Maybe we can play some games together sometime. <EOS>\n",
      "<SOS> That would be fun! <EOS>\n",
      "<SOS> Great! I'll send you my Steam name. <EOS>\n",
      "<SOS> Ok, sounds good. <EOS>\n",
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adnan\\AppData\\Local\\Temp\\ipykernel_2072\\3633430242.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  conv=row[1][2]\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for row in train.iterrows():\n",
    "    if count==1:\n",
    "        break\n",
    "    conv=row[1][2]\n",
    "    conv=conv.split(\"\\n\")\n",
    "    for i in range(0,len(conv),2):\n",
    "        print(\"<SOS> \"+conv[i][8:]+\" <EOS>\")\n",
    "        if i+1<len(conv):\n",
    "            print(\"<SOS> \"+conv[i+1][8:]+\" <EOS>\")\n",
    "    if len(conv)%2!=0:\n",
    "        print(len(conv))\n",
    "    count+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adnan\\AppData\\Local\\Temp\\ipykernel_2072\\2890846682.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  conv=row[1][2]\n"
     ]
    }
   ],
   "source": [
    "person1_train=[]\n",
    "person2_train=[]\n",
    "for row in train.iterrows():\n",
    "    conv=row[1][2]\n",
    "    conv=conv.split(\"\\n\")\n",
    "    if len(conv)%2 == 0:\n",
    "        for i in range(0,len(conv),2):\n",
    "            person1_train.append(\"<SOS> \"+conv[i][8:]+\" <EOS>\")\n",
    "            if i+1<len(conv):\n",
    "                person2_train.append(\"<SOS> \"+conv[i+1][8:]+\" <EOS>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66455, 66455)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(person1_train),len(person2_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adnan\\AppData\\Local\\Temp\\ipykernel_2072\\1743355849.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  conv=row[1][2]\n"
     ]
    }
   ],
   "source": [
    "person1_vald=[]\n",
    "person2_vald=[]\n",
    "for row in vald.iterrows():\n",
    "    conv=row[1][2]\n",
    "    conv=conv.split(\"\\n\")\n",
    "    if len(conv)%2 == 0:\n",
    "        for i in range(0,len(conv),2):\n",
    "            person1_vald.append(\"<SOS> \"+conv[i][8:]+\" <EOS>\")\n",
    "            if i+1<len(conv):\n",
    "                person2_vald.append(\"<SOS> \"+conv[i+1][8:]+\" <EOS>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7824, 7824)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(person1_vald),len(person2_vald)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = person1_train + person2_train\n",
    "\n",
    "tokenizer.fit_on_texts(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer_1.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "person1_sequences = tokenizer.texts_to_sequences(person1_train)\n",
    "person2_sequences = tokenizer.texts_to_sequences(person2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 49, 1], [2, 35, 120, 1]]\n",
      "[[2, 190, 1], [2, 54, 105, 86, 495, 57, 1]]\n"
     ]
    }
   ],
   "source": [
    "# Example of the first few tokenized sequences\n",
    "print(person1_sequences[:2])\n",
    "print(person2_sequences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 9982\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print(\"Vocabulary size:\", len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eos': 1,\n",
       " 'sos': 2,\n",
       " 'i': 3,\n",
       " 'to': 4,\n",
       " 'a': 5,\n",
       " 'you': 6,\n",
       " \"i'm\": 7,\n",
       " 'like': 8,\n",
       " 'do': 9,\n",
       " 'the': 10,\n",
       " 'and': 11,\n",
       " 'of': 12,\n",
       " 'too': 13,\n",
       " 'love': 14,\n",
       " 'it': 15,\n",
       " 'what': 16,\n",
       " 'so': 17,\n",
       " \"it's\": 18,\n",
       " \"that's\": 19,\n",
       " 'for': 20,\n",
       " 'that': 21,\n",
       " 'my': 22,\n",
       " 'fun': 23,\n",
       " 'great': 24,\n",
       " 'favorite': 25,\n",
       " 'me': 26,\n",
       " 'your': 27,\n",
       " 'is': 28,\n",
       " 'have': 29,\n",
       " \"i've\": 30,\n",
       " 'but': 31,\n",
       " 'in': 32,\n",
       " 'are': 33,\n",
       " 'oh': 34,\n",
       " \"what's\": 35,\n",
       " 'lot': 36,\n",
       " 'good': 37,\n",
       " 'about': 38,\n",
       " 'really': 39,\n",
       " 'go': 40,\n",
       " 'sounds': 41,\n",
       " 'cool': 42,\n",
       " 'sure': 43,\n",
       " 'name': 44,\n",
       " 'with': 45,\n",
       " 'be': 46,\n",
       " 'read': 47,\n",
       " 'play': 48,\n",
       " 'hi': 49,\n",
       " 'thanks': 50,\n",
       " 'always': 51,\n",
       " 'should': 52,\n",
       " 'nice': 53,\n",
       " 'not': 54,\n",
       " 'also': 55,\n",
       " 'can': 56,\n",
       " 'out': 57,\n",
       " 'work': 58,\n",
       " \"they're\": 59,\n",
       " 'games': 60,\n",
       " 'been': 61,\n",
       " 'all': 62,\n",
       " 'time': 63,\n",
       " 'we': 64,\n",
       " \"i'll\": 65,\n",
       " 'know': 66,\n",
       " 'think': 67,\n",
       " 'user': 68,\n",
       " 'yeah': 69,\n",
       " 'glad': 70,\n",
       " 'movies': 71,\n",
       " 'was': 72,\n",
       " 'music': 73,\n",
       " 'on': 74,\n",
       " 'things': 75,\n",
       " 'them': 76,\n",
       " 'at': 77,\n",
       " 'kind': 78,\n",
       " 'one': 79,\n",
       " 'never': 80,\n",
       " 'try': 81,\n",
       " 'books': 82,\n",
       " 'meet': 83,\n",
       " 'new': 84,\n",
       " 'some': 85,\n",
       " 'just': 86,\n",
       " 'going': 87,\n",
       " 'video': 88,\n",
       " 'get': 89,\n",
       " 'those': 90,\n",
       " 'there': 91,\n",
       " 'watch': 92,\n",
       " 'awesome': 93,\n",
       " \"you're\": 94,\n",
       " 'see': 95,\n",
       " 'well': 96,\n",
       " 'book': 97,\n",
       " 'way': 98,\n",
       " 'how': 99,\n",
       " 'wanted': 100,\n",
       " 'would': 101,\n",
       " 'people': 102,\n",
       " 'job': 103,\n",
       " 'will': 104,\n",
       " 'much': 105,\n",
       " 'sometime': 106,\n",
       " 'agree': 107,\n",
       " 'thing': 108,\n",
       " 'doing': 109,\n",
       " 'make': 110,\n",
       " 'best': 111,\n",
       " 'from': 112,\n",
       " 'friends': 113,\n",
       " 'different': 114,\n",
       " 'they': 115,\n",
       " 'interesting': 116,\n",
       " 'living': 117,\n",
       " 'right': 118,\n",
       " 'very': 119,\n",
       " 'up': 120,\n",
       " 'amazing': 121,\n",
       " 'heard': 122,\n",
       " 'movie': 123,\n",
       " 'hope': 124,\n",
       " 'when': 125,\n",
       " 'definitely': 126,\n",
       " \"don't\": 127,\n",
       " 'food': 128,\n",
       " 'game': 129,\n",
       " 'being': 130,\n",
       " 'hello': 131,\n",
       " 'by': 132,\n",
       " 'check': 133,\n",
       " 'no': 134,\n",
       " 'family': 135,\n",
       " 'else': 136,\n",
       " 'learn': 137,\n",
       " 'beautiful': 138,\n",
       " 'fan': 139,\n",
       " 'big': 140,\n",
       " 'enjoy': 141,\n",
       " 'series': 142,\n",
       " 'cook': 143,\n",
       " 'as': 144,\n",
       " \"i'd\": 145,\n",
       " 'any': 146,\n",
       " 'hiking': 147,\n",
       " \"you'll\": 148,\n",
       " 'looking': 149,\n",
       " 'fiction': 150,\n",
       " 'working': 151,\n",
       " 'an': 152,\n",
       " 'reading': 153,\n",
       " 'could': 154,\n",
       " 'who': 155,\n",
       " 'before': 156,\n",
       " 'want': 157,\n",
       " 'playing': 158,\n",
       " 'kinds': 159,\n",
       " 'few': 160,\n",
       " 'something': 161,\n",
       " 'day': 162,\n",
       " 'world': 163,\n",
       " '2': 164,\n",
       " 'now': 165,\n",
       " 'problem': 166,\n",
       " 'hear': 167,\n",
       " 'might': 168,\n",
       " 'spend': 169,\n",
       " 'yet': 170,\n",
       " 'thank': 171,\n",
       " '1': 172,\n",
       " 'into': 173,\n",
       " 'maybe': 174,\n",
       " 'this': 175,\n",
       " \"he's\": 176,\n",
       " 'kids': 177,\n",
       " 'their': 178,\n",
       " 'where': 179,\n",
       " 'live': 180,\n",
       " 'talking': 181,\n",
       " 'more': 182,\n",
       " 'if': 183,\n",
       " 'teacher': 184,\n",
       " \"2's\": 185,\n",
       " 'rewarding': 186,\n",
       " 'write': 187,\n",
       " 'hard': 188,\n",
       " 'imagine': 189,\n",
       " 'hey': 190,\n",
       " 'okay': 191,\n",
       " 'dog': 192,\n",
       " 'or': 193,\n",
       " 'find': 194,\n",
       " 'song': 195,\n",
       " 'favorites': 196,\n",
       " 'hang': 197,\n",
       " 'listen': 198,\n",
       " 'dogs': 199,\n",
       " 'travel': 200,\n",
       " 'eat': 201,\n",
       " 'help': 202,\n",
       " \"1's\": 203,\n",
       " 'school': 204,\n",
       " 'band': 205,\n",
       " 'used': 206,\n",
       " 'beach': 207,\n",
       " 'animals': 208,\n",
       " 'wow': 209,\n",
       " 'other': 210,\n",
       " 'place': 211,\n",
       " 'historical': 212,\n",
       " 'excited': 213,\n",
       " 'part': 214,\n",
       " 'country': 215,\n",
       " 'especially': 216,\n",
       " 'walks': 217,\n",
       " 'ever': 218,\n",
       " 'together': 219,\n",
       " 'give': 220,\n",
       " 'rock': 221,\n",
       " 'interested': 222,\n",
       " 'life': 223,\n",
       " 'show': 224,\n",
       " 'happy': 225,\n",
       " 'weekend': 226,\n",
       " 'watching': 227,\n",
       " 'had': 228,\n",
       " 'trying': 229,\n",
       " 'pretty': 230,\n",
       " 'both': 231,\n",
       " 'someone': 232,\n",
       " 'able': 233,\n",
       " 'fantasy': 234,\n",
       " 'getting': 235,\n",
       " 'seen': 236,\n",
       " 'city': 237,\n",
       " 'come': 238,\n",
       " 'yes': 239,\n",
       " 'delicious': 240,\n",
       " 'relax': 241,\n",
       " 'camping': 242,\n",
       " 'probably': 243,\n",
       " 'own': 244,\n",
       " 'feeling': 245,\n",
       " 'start': 246,\n",
       " 'free': 247,\n",
       " 'fishing': 248,\n",
       " 'making': 249,\n",
       " 'am': 250,\n",
       " 'science': 251,\n",
       " 'bands': 252,\n",
       " 'years': 253,\n",
       " 'common': 254,\n",
       " 'he': 255,\n",
       " 'hobbies': 256,\n",
       " 'times': 257,\n",
       " 'met': 258,\n",
       " 'take': 259,\n",
       " 'still': 260,\n",
       " 'bet': 261,\n",
       " 'park': 262,\n",
       " 'novels': 263,\n",
       " 'keep': 264,\n",
       " 'recommendation': 265,\n",
       " \"we're\": 266,\n",
       " 'learning': 267,\n",
       " 'places': 268,\n",
       " 'home': 269,\n",
       " 'here': 270,\n",
       " 'welcome': 271,\n",
       " 'such': 272,\n",
       " 'two': 273,\n",
       " 'important': 274,\n",
       " 'visit': 275,\n",
       " 'history': 276,\n",
       " 'appreciate': 277,\n",
       " 'color': 278,\n",
       " 'teach': 279,\n",
       " 'must': 280,\n",
       " 'concerts': 281,\n",
       " 'pizza': 282,\n",
       " 'funny': 283,\n",
       " 'action': 284,\n",
       " 'stay': 285,\n",
       " 'tv': 286,\n",
       " 'long': 287,\n",
       " 'did': 288,\n",
       " 'played': 289,\n",
       " 'pasta': 290,\n",
       " 'person': 291,\n",
       " 'mean': 292,\n",
       " 'draw': 293,\n",
       " 'relaxing': 294,\n",
       " 'her': 295,\n",
       " 'today': 296,\n",
       " 'art': 297,\n",
       " 'story': 298,\n",
       " 'many': 299,\n",
       " \"she's\": 300,\n",
       " 'grow': 301,\n",
       " 'feel': 302,\n",
       " 'swimming': 303,\n",
       " 'classic': 304,\n",
       " 'seeing': 305,\n",
       " 'perfect': 306,\n",
       " 'next': 307,\n",
       " 'studying': 308,\n",
       " 'mostly': 309,\n",
       " 'look': 310,\n",
       " 'sound': 311,\n",
       " 'characters': 312,\n",
       " 'again': 313,\n",
       " \"can't\": 314,\n",
       " 'forward': 315,\n",
       " 'college': 316,\n",
       " 'software': 317,\n",
       " 'most': 318,\n",
       " 'little': 319,\n",
       " 'thinking': 320,\n",
       " 'shows': 321,\n",
       " 'myself': 322,\n",
       " 'cute': 323,\n",
       " 'talk': 324,\n",
       " 'over': 325,\n",
       " 'same': 326,\n",
       " 'need': 327,\n",
       " \"you'd\": 328,\n",
       " 'pets': 329,\n",
       " 'why': 330,\n",
       " 'cats': 331,\n",
       " 'run': 332,\n",
       " 'makes': 333,\n",
       " 'better': 334,\n",
       " 'experience': 335,\n",
       " 'his': 336,\n",
       " 'spending': 337,\n",
       " 'cat': 338,\n",
       " 'helping': 339,\n",
       " 'golden': 340,\n",
       " 'sing': 341,\n",
       " 'every': 342,\n",
       " 'hike': 343,\n",
       " 'harry': 344,\n",
       " 'guitar': 345,\n",
       " 'exercise': 346,\n",
       " 'got': 347,\n",
       " 'say': 348,\n",
       " 'lord': 349,\n",
       " 'mom': 350,\n",
       " 'paint': 351,\n",
       " 'rings': 352,\n",
       " 'back': 353,\n",
       " 'bad': 354,\n",
       " 'potter': 355,\n",
       " 'worth': 356,\n",
       " 'engineer': 357,\n",
       " 'team': 358,\n",
       " 'idea': 359,\n",
       " 'enjoying': 360,\n",
       " 'john': 361,\n",
       " 'tough': 362,\n",
       " 'basketball': 363,\n",
       " 'wish': 364,\n",
       " 'exciting': 365,\n",
       " 'mountains': 366,\n",
       " 'old': 367,\n",
       " 'nurse': 368,\n",
       " 'concert': 369,\n",
       " 'plans': 370,\n",
       " 'soon': 371,\n",
       " 'local': 372,\n",
       " 'year': 373,\n",
       " 'him': 374,\n",
       " 'bake': 375,\n",
       " 'writing': 376,\n",
       " 'fish': 377,\n",
       " 'shopping': 378,\n",
       " 'mystery': 379,\n",
       " 'anything': 380,\n",
       " 'guess': 381,\n",
       " 'let': 382,\n",
       " 'nature': 383,\n",
       " 'interests': 384,\n",
       " 'dance': 385,\n",
       " 'made': 386,\n",
       " 'loved': 387,\n",
       " 'business': 388,\n",
       " \"you've\": 389,\n",
       " 'future': 390,\n",
       " 'football': 391,\n",
       " 'around': 392,\n",
       " 'named': 393,\n",
       " 'either': 394,\n",
       " 'sports': 395,\n",
       " \"haven't\": 396,\n",
       " 'she': 397,\n",
       " 'outdoors': 398,\n",
       " 'italian': 399,\n",
       " 'sarah': 400,\n",
       " 'meeting': 401,\n",
       " \"there's\": 402,\n",
       " 'first': 403,\n",
       " 'thought': 404,\n",
       " 'outside': 405,\n",
       " \"won't\": 406,\n",
       " 'stories': 407,\n",
       " 'plan': 408,\n",
       " 'vegan': 409,\n",
       " 'easy': 410,\n",
       " 'yoga': 411,\n",
       " 'understand': 412,\n",
       " 'only': 413,\n",
       " 'challenge': 414,\n",
       " 'animal': 415,\n",
       " 'then': 416,\n",
       " 'grade': 417,\n",
       " 'ride': 418,\n",
       " 'mind': 419,\n",
       " 'challenging': 420,\n",
       " 'busy': 421,\n",
       " 'after': 422,\n",
       " 'our': 423,\n",
       " 'small': 424,\n",
       " 'yourself': 425,\n",
       " 'garden': 426,\n",
       " 'started': 427,\n",
       " 'chocolate': 428,\n",
       " 'horror': 429,\n",
       " 'though': 430,\n",
       " 'high': 431,\n",
       " 'soccer': 432,\n",
       " 'anime': 433,\n",
       " 'cooking': 434,\n",
       " 'online': 435,\n",
       " 'cars': 436,\n",
       " 'novel': 437,\n",
       " 'variety': 438,\n",
       " 'swim': 439,\n",
       " 'actually': 440,\n",
       " 'adventure': 441,\n",
       " 'gym': 442,\n",
       " 'difference': 443,\n",
       " 'open': 444,\n",
       " 'genres': 445,\n",
       " 'clothes': 446,\n",
       " 'seem': 447,\n",
       " 'student': 448,\n",
       " 'california': 449,\n",
       " 'names': 450,\n",
       " 'create': 451,\n",
       " 'beatles': 452,\n",
       " 'wait': 453,\n",
       " 'running': 454,\n",
       " 'than': 455,\n",
       " 'sorry': 456,\n",
       " 'york': 457,\n",
       " 'sense': 458,\n",
       " 'since': 459,\n",
       " 'figure': 460,\n",
       " 'retriever': 461,\n",
       " 'cultures': 462,\n",
       " 'has': 463,\n",
       " 'comedies': 464,\n",
       " 'currently': 465,\n",
       " 'explore': 466,\n",
       " \"let's\": 467,\n",
       " 'bike': 468,\n",
       " 'songs': 469,\n",
       " 'while': 470,\n",
       " 'usually': 471,\n",
       " 'creative': 472,\n",
       " 'zelda': 473,\n",
       " 'friendly': 474,\n",
       " 'peaceful': 475,\n",
       " 'tried': 476,\n",
       " 'does': 477,\n",
       " 'store': 478,\n",
       " 'passionate': 479,\n",
       " 'having': 480,\n",
       " 'later': 481,\n",
       " 'listening': 482,\n",
       " 'dish': 483,\n",
       " 'were': 484,\n",
       " 'everything': 485,\n",
       " 'someday': 486,\n",
       " 'car': 487,\n",
       " 'lives': 488,\n",
       " 'done': 489,\n",
       " 'taking': 490,\n",
       " 'countries': 491,\n",
       " 'piano': 492,\n",
       " 'season': 493,\n",
       " 'bye': 494,\n",
       " 'hanging': 495,\n",
       " 'mario': 496,\n",
       " 'lucky': 497,\n",
       " 'everyone': 498,\n",
       " 'baseball': 499,\n",
       " 'rpgs': 500,\n",
       " 'kid': 501,\n",
       " 'friend': 502,\n",
       " 'younger': 503,\n",
       " 'drive': 504,\n",
       " 'writer': 505,\n",
       " 'house': 506,\n",
       " 'three': 507,\n",
       " 'exactly': 508,\n",
       " 'short': 509,\n",
       " 'volunteer': 510,\n",
       " 'choice': 511,\n",
       " 'sometimes': 512,\n",
       " 'enough': 513,\n",
       " 'tell': 514,\n",
       " 'shop': 515,\n",
       " 'fresh': 516,\n",
       " 'dancing': 517,\n",
       " 'practice': 518,\n",
       " 'put': 519,\n",
       " 'enjoyed': 520,\n",
       " 'lasagna': 521,\n",
       " 'jazz': 522,\n",
       " 'far': 523,\n",
       " 'ways': 524,\n",
       " 'use': 525,\n",
       " 'true': 526,\n",
       " 'asking': 527,\n",
       " 'europe': 528,\n",
       " 'types': 529,\n",
       " 'red': 530,\n",
       " 'regret': 531,\n",
       " 'restaurant': 532,\n",
       " 'player': 533,\n",
       " 'catch': 534,\n",
       " 'lately': 535,\n",
       " 'bit': 536,\n",
       " 'night': 537,\n",
       " 'ready': 538,\n",
       " 'hours': 539,\n",
       " 'artist': 540,\n",
       " 'written': 541,\n",
       " 'farm': 542,\n",
       " 'sorts': 543,\n",
       " 'us': 544,\n",
       " 'through': 545,\n",
       " 'once': 546,\n",
       " 'looks': 547,\n",
       " 'walk': 548,\n",
       " 'hoping': 549,\n",
       " 'pictures': 550,\n",
       " 'dream': 551,\n",
       " 'similar': 552,\n",
       " 'recipes': 553,\n",
       " 'law': 554,\n",
       " 'videos': 555,\n",
       " 'water': 556,\n",
       " 'bring': 557,\n",
       " 'another': 558,\n",
       " 'skiing': 559,\n",
       " 'shape': 560,\n",
       " 'mine': 561,\n",
       " 'redemption': 562,\n",
       " 'finished': 563,\n",
       " 'dishes': 564,\n",
       " 'week': 565,\n",
       " 'build': 566,\n",
       " 'baking': 567,\n",
       " 'artists': 568,\n",
       " 'stress': 569,\n",
       " 'teaching': 570,\n",
       " 'character': 571,\n",
       " 'because': 572,\n",
       " 'sweet': 573,\n",
       " 'doctor': 574,\n",
       " 'town': 575,\n",
       " 'suggestion': 576,\n",
       " 'list': 577,\n",
       " 'cookies': 578,\n",
       " 'advice': 579,\n",
       " 'computer': 580,\n",
       " 'painting': 581,\n",
       " 'called': 582,\n",
       " 'war': 583,\n",
       " 'field': 584,\n",
       " 'role': 585,\n",
       " 'metal': 586,\n",
       " 'canada': 587,\n",
       " 'number': 588,\n",
       " 'pick': 589,\n",
       " 'money': 590,\n",
       " 'coffee': 591,\n",
       " 'pop': 592,\n",
       " 'company': 593,\n",
       " 'luck': 594,\n",
       " 'talented': 595,\n",
       " 'down': 596,\n",
       " 'children': 597,\n",
       " 'retrievers': 598,\n",
       " 'summer': 599,\n",
       " 'ago': 600,\n",
       " 'shawshank': 601,\n",
       " 'genre': 602,\n",
       " 'office': 603,\n",
       " 'huge': 604,\n",
       " 'romance': 605,\n",
       " '10': 606,\n",
       " 'planning': 607,\n",
       " 'hair': 608,\n",
       " 'last': 609,\n",
       " 'kindergarten': 610,\n",
       " 'taste': 611,\n",
       " 'girl': 612,\n",
       " 'tennis': 613,\n",
       " 'type': 614,\n",
       " 'went': 615,\n",
       " 'yours': 616,\n",
       " 'neither': 617,\n",
       " 'lost': 618,\n",
       " 'weather': 619,\n",
       " 'chance': 620,\n",
       " 'meaning': 621,\n",
       " 'gatsby': 622,\n",
       " 'text': 623,\n",
       " 'care': 624,\n",
       " 'comic': 625,\n",
       " 'even': 626,\n",
       " 'classical': 627,\n",
       " 'culture': 628,\n",
       " 'anyone': 629,\n",
       " 'comedy': 630,\n",
       " 'professional': 631,\n",
       " 'call': 632,\n",
       " 'off': 633,\n",
       " 'authors': 634,\n",
       " 'study': 635,\n",
       " 'chef': 636,\n",
       " 'share': 637,\n",
       " 'italy': 638,\n",
       " 'black': 639,\n",
       " 'sci': 640,\n",
       " 'fi': 641,\n",
       " 'community': 642,\n",
       " 'often': 643,\n",
       " 'decide': 644,\n",
       " 'exploring': 645,\n",
       " 'sauce': 646,\n",
       " 'marketing': 647,\n",
       " 'state': 648,\n",
       " 'mary': 649,\n",
       " 'along': 650,\n",
       " 'question': 651,\n",
       " 'drink': 652,\n",
       " 'vegetables': 653,\n",
       " 'loves': 654,\n",
       " 'trip': 655,\n",
       " 'hockey': 656,\n",
       " 'blue': 657,\n",
       " 'kart': 658,\n",
       " 'japan': 659,\n",
       " 'skyrim': 660,\n",
       " 'ice': 661,\n",
       " 'least': 662,\n",
       " 'send': 663,\n",
       " 'impressive': 664,\n",
       " 'museums': 665,\n",
       " 'goal': 666,\n",
       " 'married': 667,\n",
       " 'de': 668,\n",
       " 'english': 669,\n",
       " 'cheese': 670,\n",
       " \"we'll\": 671,\n",
       " 'recently': 672,\n",
       " 'coming': 673,\n",
       " 'traveling': 674,\n",
       " 'express': 675,\n",
       " '3': 676,\n",
       " 'comics': 677,\n",
       " 'liked': 678,\n",
       " 'healthy': 679,\n",
       " 'head': 680,\n",
       " 'king': 681,\n",
       " 'scary': 682,\n",
       " 'fast': 683,\n",
       " 'stones': 684,\n",
       " 'ones': 685,\n",
       " 'each': 686,\n",
       " 'blues': 687,\n",
       " 'racing': 688,\n",
       " 'saw': 689,\n",
       " 'buddy': 690,\n",
       " 'projects': 691,\n",
       " 'jealous': 692,\n",
       " 'hunting': 693,\n",
       " 'active': 694,\n",
       " 'rolling': 695,\n",
       " 'legend': 696,\n",
       " 'become': 697,\n",
       " 'reason': 698,\n",
       " 'away': 699,\n",
       " 'surf': 700,\n",
       " 'lake': 701,\n",
       " 'pet': 702,\n",
       " 'graduate': 703,\n",
       " 'change': 704,\n",
       " 'weekends': 705,\n",
       " 'likes': 706,\n",
       " 'collect': 707,\n",
       " 'laugh': 708,\n",
       " 'real': 709,\n",
       " 'ocean': 710,\n",
       " 'french': 711,\n",
       " 'green': 712,\n",
       " 'mysteries': 713,\n",
       " 'special': 714,\n",
       " 'career': 715,\n",
       " 'thrones': 716,\n",
       " 'landscapes': 717,\n",
       " 'museum': 718,\n",
       " 'building': 719,\n",
       " 'walking': 720,\n",
       " 'whole': 721,\n",
       " 'air': 722,\n",
       " 'recommend': 723,\n",
       " 'civil': 724,\n",
       " 'horses': 725,\n",
       " 'states': 726,\n",
       " 'freedom': 727,\n",
       " 'star': 728,\n",
       " 'r': 729,\n",
       " 'grew': 730,\n",
       " 'race': 731,\n",
       " \"people's\": 732,\n",
       " 'cake': 733,\n",
       " 'set': 734,\n",
       " 'starting': 735,\n",
       " 'clear': 736,\n",
       " 'fascinating': 737,\n",
       " 'finding': 738,\n",
       " 'days': 739,\n",
       " 'biking': 740,\n",
       " 'duty': 741,\n",
       " 'sport': 742,\n",
       " 'hobby': 743,\n",
       " 'riding': 744,\n",
       " 'move': 745,\n",
       " 'hop': 746,\n",
       " 'eating': 747,\n",
       " 'five': 748,\n",
       " 'takes': 749,\n",
       " 'degree': 750,\n",
       " 'husband': 751,\n",
       " 'smart': 752,\n",
       " 'anymore': 753,\n",
       " 'truck': 754,\n",
       " 'foods': 755,\n",
       " 'drawing': 756,\n",
       " 'choose': 757,\n",
       " 'others': 758,\n",
       " 'spare': 759,\n",
       " 'violin': 760,\n",
       " 'class': 761,\n",
       " 'near': 762,\n",
       " 'romantic': 763,\n",
       " 'pepperoni': 764,\n",
       " 'meat': 765,\n",
       " 'dinner': 766,\n",
       " 'gardening': 767,\n",
       " 'trilogy': 768,\n",
       " 'united': 769,\n",
       " 'woman': 770,\n",
       " 'winter': 771,\n",
       " 'vegetarian': 772,\n",
       " 'makeup': 773,\n",
       " 'project': 774,\n",
       " 'mother': 775,\n",
       " 'username': 776,\n",
       " 'ballet': 777,\n",
       " 'plants': 778,\n",
       " 'driver': 779,\n",
       " 'shares': 780,\n",
       " 'buy': 781,\n",
       " 'stephen': 782,\n",
       " 'exchange': 783,\n",
       " 'found': 784,\n",
       " 'hip': 785,\n",
       " 'position': 786,\n",
       " 'numbers': 787,\n",
       " 'musician': 788,\n",
       " 'poetry': 789,\n",
       " 'multiple': 790,\n",
       " 'agatha': 791,\n",
       " 'j': 792,\n",
       " 'worked': 793,\n",
       " 'escape': 794,\n",
       " 'young': 795,\n",
       " 'rap': 796,\n",
       " 'jane': 797,\n",
       " 'cakes': 798,\n",
       " 'wear': 799,\n",
       " 'social': 800,\n",
       " 'conversation': 801,\n",
       " 'group': 802,\n",
       " 'chat': 803,\n",
       " 'offer': 804,\n",
       " 'unique': 805,\n",
       " 'problems': 806,\n",
       " 'american': 807,\n",
       " 'research': 808,\n",
       " 'restaurants': 809,\n",
       " 'add': 810,\n",
       " 'youtube': 811,\n",
       " 'dad': 812,\n",
       " 'congratulations': 813,\n",
       " 'outlander': 814,\n",
       " 'chip': 815,\n",
       " 'woods': 816,\n",
       " 'christie': 817,\n",
       " 'moved': 818,\n",
       " 'sell': 819,\n",
       " 'party': 820,\n",
       " 'roll': 821,\n",
       " 'grocery': 822,\n",
       " 'students': 823,\n",
       " 'sushi': 824,\n",
       " 'cold': 825,\n",
       " 'encouragement': 826,\n",
       " 'national': 827,\n",
       " 'theater': 828,\n",
       " 'shelter': 829,\n",
       " 'scenery': 830,\n",
       " 'flowers': 831,\n",
       " 'close': 832,\n",
       " 'space': 833,\n",
       " 'besides': 834,\n",
       " 'spaghetti': 835,\n",
       " '5': 836,\n",
       " 'hands': 837,\n",
       " 'lawyer': 838,\n",
       " 'personal': 839,\n",
       " 'chatting': 840,\n",
       " 'growing': 841,\n",
       " 'paintings': 842,\n",
       " 'paris': 843,\n",
       " 'scott': 844,\n",
       " 'energy': 845,\n",
       " 'child': 846,\n",
       " \"didn't\": 847,\n",
       " 'surfing': 848,\n",
       " 'siblings': 849,\n",
       " 'bass': 850,\n",
       " 'industry': 851,\n",
       " 'disappointed': 852,\n",
       " 'dead': 853,\n",
       " 'd': 854,\n",
       " 'cream': 855,\n",
       " 'inspiring': 856,\n",
       " 'experiences': 857,\n",
       " 'mountain': 858,\n",
       " 'loving': 859,\n",
       " 'smell': 860,\n",
       " 'afraid': 861,\n",
       " \"how's\": 862,\n",
       " 'retired': 863,\n",
       " 'whatever': 864,\n",
       " 'fly': 865,\n",
       " 'parks': 866,\n",
       " 'powerful': 867,\n",
       " 'late': 868,\n",
       " 'quiet': 869,\n",
       " 'indie': 870,\n",
       " 'man': 871,\n",
       " 'alex': 872,\n",
       " 'classes': 873,\n",
       " 'activities': 874,\n",
       " 'tip': 875,\n",
       " 'photography': 876,\n",
       " 'parents': 877,\n",
       " 'wedding': 878,\n",
       " 'proud': 879,\n",
       " 'fitzgerald': 880,\n",
       " 'impressed': 881,\n",
       " 'san': 882,\n",
       " 'chicken': 883,\n",
       " 'earth': 884,\n",
       " 'florida': 885,\n",
       " 'grand': 886,\n",
       " 'environment': 887,\n",
       " 'hikes': 888,\n",
       " 'satisfying': 889,\n",
       " 'top': 890,\n",
       " 'design': 891,\n",
       " 'michigan': 892,\n",
       " 'age': 893,\n",
       " 'sew': 894,\n",
       " 'goals': 895,\n",
       " 'singer': 896,\n",
       " 'led': 897,\n",
       " 'boy': 898,\n",
       " 'superhero': 899,\n",
       " 'board': 900,\n",
       " 'sad': 901,\n",
       " 'wild': 902,\n",
       " 'subject': 903,\n",
       " 'zeppelin': 904,\n",
       " 'learned': 905,\n",
       " 'dark': 906,\n",
       " 'ski': 907,\n",
       " 'sister': 908,\n",
       " 'drums': 909,\n",
       " 'shoes': 910,\n",
       " 'accountant': 911,\n",
       " 'four': 912,\n",
       " 'wife': 913,\n",
       " 'seafood': 914,\n",
       " 'killers': 915,\n",
       " 'train': 916,\n",
       " 'witcher': 917,\n",
       " 'designer': 918,\n",
       " 'area': 919,\n",
       " 'driving': 920,\n",
       " 'texas': 921,\n",
       " 'f': 922,\n",
       " 'works': 923,\n",
       " 'math': 924,\n",
       " 'stuff': 925,\n",
       " 'beer': 926,\n",
       " 'pink': 927,\n",
       " 'final': 928,\n",
       " 'save': 929,\n",
       " 'major': 930,\n",
       " 'cartoons': 931,\n",
       " 'piece': 932,\n",
       " 'crime': 933,\n",
       " 'colorado': 934,\n",
       " 'episode': 935,\n",
       " 'trail': 936,\n",
       " 'brother': 937,\n",
       " 'creating': 938,\n",
       " 'son': 939,\n",
       " 'believe': 940,\n",
       " 'wars': 941,\n",
       " 'mexico': 942,\n",
       " 'which': 943,\n",
       " 'horse': 944,\n",
       " 'voice': 945,\n",
       " 'full': 946,\n",
       " 'jewelry': 947,\n",
       " 'rest': 948,\n",
       " 'nonfiction': 949,\n",
       " 'thrillers': 950,\n",
       " 'chicago': 951,\n",
       " 'flower': 952,\n",
       " 'saturday': 953,\n",
       " 'london': 954,\n",
       " 'death': 955,\n",
       " 'calming': 956,\n",
       " 'sun': 957,\n",
       " 'computers': 958,\n",
       " 'minecraft': 959,\n",
       " 'recipe': 960,\n",
       " 'diana': 961,\n",
       " 'twice': 962,\n",
       " 'south': 963,\n",
       " 'climbing': 964,\n",
       " 'hunger': 965,\n",
       " 'miss': 966,\n",
       " \"who's\": 967,\n",
       " 'gabaldon': 968,\n",
       " 'rpg': 969,\n",
       " 'guide': 970,\n",
       " 'charlie': 971,\n",
       " 'goes': 972,\n",
       " 'style': 973,\n",
       " 'language': 974,\n",
       " 'kayaking': 975,\n",
       " 'brothers': 976,\n",
       " 'hospital': 977,\n",
       " 'princess': 978,\n",
       " 'club': 979,\n",
       " 'mexican': 980,\n",
       " 'catchy': 981,\n",
       " 'librarian': 982,\n",
       " 'end': 983,\n",
       " 'university': 984,\n",
       " 'worry': 985,\n",
       " 'simple': 986,\n",
       " 'sleep': 987,\n",
       " 'worlds': 988,\n",
       " 'galaxy': 989,\n",
       " 'hot': 990,\n",
       " 'folk': 991,\n",
       " 'views': 992,\n",
       " 'strong': 993,\n",
       " 'opera': 994,\n",
       " 'answer': 995,\n",
       " 'brave': 996,\n",
       " 'moving': 997,\n",
       " \"hitchhiker's\": 998,\n",
       " 'bar': 999,\n",
       " 'tacos': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data=person1_sequences\n",
    "decoder_data=person2_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_data = [sent[:-1] for sent in decoder_data]\n",
    "\n",
    "decoder_target_data = [sent[1:] for sent in decoder_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = max(max(len(seq) for seq in person1_sequences),\n",
    "                     max(len(seq) for seq in person2_sequences))\n",
    "max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data_padded = pad_sequences(encoder_input_data, maxlen=max_seq_length, padding='post')\n",
    "decoder_input_data_padded= pad_sequences(decoder_input_data, maxlen=max_seq_length, padding='post')\n",
    "decoder_target_data_padedd = pad_sequences(decoder_target_data, maxlen=max_seq_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape decoder_target_data to make it suitable for sparse_categorical_crossentropy loss\n",
    "decoder_target_data_padedd = np.expand_dims(decoder_target_data_padedd, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input_data.shape: (66455, 137)\n",
      "decoder_input_data.shape: (66455, 137)\n",
      "decoder_target_data.shape: (66455, 137, 1)\n"
     ]
    }
   ],
   "source": [
    "# Check the shapes again\n",
    "print(f\"encoder_input_data.shape: {encoder_input_data_padded.shape}\")\n",
    "print(f\"decoder_input_data.shape: {decoder_input_data_padded.shape}\")\n",
    "print(f\"decoder_target_data.shape: {decoder_target_data_padedd.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_person1_sequences = tokenizer.texts_to_sequences(person1_vald)\n",
    "validation_person2_sequences = tokenizer.texts_to_sequences(person2_vald)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data_valda=validation_person1_sequences\n",
    "decoder_data_valda=validation_person2_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_data_valda=[sent[:-1]for sent in decoder_data_valda]\n",
    "decoder_target_data_valda=[sent[1:]for sent in decoder_data_valda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length_validation = max(max(len(seq) for seq in validation_person1_sequences),\n",
    "                     max(len(seq) for seq in validation_person2_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data_val_padded = pad_sequences(encoder_input_data_valda, maxlen=max_seq_length_validation, padding='post')\n",
    "decoder_input_data_val_padded = pad_sequences(decoder_input_data_valda, maxlen=max_seq_length_validation, padding='post')\n",
    "decoder_target_data_val_padded = pad_sequences(decoder_target_data_valda, maxlen=max_seq_length_validation, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape decoder_target_data to make it suitable for sparse_categorical_crossentropy loss\n",
    "decoder_target_data_val_padded = np.expand_dims(decoder_target_data_val_padded, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input_data_validation.shape: (7824, 61)\n",
      "decoder_input_data_validation.shape: (7824, 61)\n",
      "decoder_target_data_validation.shape: (7824, 61, 1)\n"
     ]
    }
   ],
   "source": [
    "# Check the shapes again\n",
    "print(f\"encoder_input_data_validation.shape: {encoder_input_data_val_padded.shape}\")\n",
    "print(f\"decoder_input_data_validation.shape: {decoder_input_data_val_padded.shape}\")\n",
    "print(f\"decoder_target_data_validation.shape: {decoder_target_data_val_padded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "embedding_dim = 1024\n",
    "latent_dim = 512\n",
    "vocab_size = len(word_index) +1\n",
    "#dropout_rate = 0.1 # dropout mae accure a problem\n",
    "bat_size = 256\n",
    "learning_rate=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, None, 1024)   10222592    ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, None, 1024)   10222592    ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 512),        3147776     ['embedding[0][0]']              \n",
      "                                 (None, 512),                                                     \n",
      "                                 (None, 512)]                                                     \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, None, 512),  3147776     ['embedding_1[0][0]',            \n",
      "                                 (None, 512),                     'lstm[0][1]',                   \n",
      "                                 (None, 512)]                     'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 9983)   5121279     ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 31,862,015\n",
      "Trainable params: 31,862,015\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "encoder_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim,mask_zero=True)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim ,return_state=True)\n",
    "_, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Define the decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim,mask_zero=True)(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim,return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer= optimizer, loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "260/260 [==============================] - 51s 170ms/step - loss: 0.3747 - acc: 0.1753 - val_loss: 0.6426 - val_acc: 0.2829\n",
      "Epoch 2/25\n",
      "260/260 [==============================] - 42s 162ms/step - loss: 0.2528 - acc: 0.3543 - val_loss: 0.5060 - val_acc: 0.3922\n",
      "Epoch 3/25\n",
      "260/260 [==============================] - 42s 161ms/step - loss: 0.2098 - acc: 0.4325 - val_loss: 0.4499 - val_acc: 0.4516\n",
      "Epoch 4/25\n",
      "260/260 [==============================] - 42s 161ms/step - loss: 0.1859 - acc: 0.4797 - val_loss: 0.4186 - val_acc: 0.4776\n",
      "Epoch 5/25\n",
      "260/260 [==============================] - 42s 161ms/step - loss: 0.1698 - acc: 0.5080 - val_loss: 0.3984 - val_acc: 0.4960\n",
      "Epoch 6/25\n",
      "260/260 [==============================] - 42s 161ms/step - loss: 0.1576 - acc: 0.5303 - val_loss: 0.3845 - val_acc: 0.5106\n",
      "Epoch 7/25\n",
      "260/260 [==============================] - 42s 161ms/step - loss: 0.1477 - acc: 0.5501 - val_loss: 0.3753 - val_acc: 0.5206\n",
      "Epoch 8/25\n",
      "260/260 [==============================] - 42s 161ms/step - loss: 0.1391 - acc: 0.5671 - val_loss: 0.3680 - val_acc: 0.5282\n",
      "Epoch 9/25\n",
      "260/260 [==============================] - 42s 161ms/step - loss: 0.1317 - acc: 0.5827 - val_loss: 0.3634 - val_acc: 0.5351\n",
      "Epoch 10/25\n",
      "260/260 [==============================] - 42s 163ms/step - loss: 0.1251 - acc: 0.5971 - val_loss: 0.3597 - val_acc: 0.5389\n",
      "Epoch 11/25\n",
      "260/260 [==============================] - 42s 162ms/step - loss: 0.1190 - acc: 0.6105 - val_loss: 0.3573 - val_acc: 0.5425\n",
      "Epoch 12/25\n",
      "260/260 [==============================] - 42s 161ms/step - loss: 0.1135 - acc: 0.6233 - val_loss: 0.3565 - val_acc: 0.5446\n",
      "Epoch 13/25\n",
      "260/260 [==============================] - 43s 164ms/step - loss: 0.1083 - acc: 0.6364 - val_loss: 0.3562 - val_acc: 0.5462\n",
      "Epoch 14/25\n",
      "260/260 [==============================] - 42s 163ms/step - loss: 0.1035 - acc: 0.6485 - val_loss: 0.3567 - val_acc: 0.5469\n",
      "Epoch 15/25\n",
      "260/260 [==============================] - 42s 162ms/step - loss: 0.0990 - acc: 0.6608 - val_loss: 0.3576 - val_acc: 0.5491\n",
      "Epoch 16/25\n",
      "260/260 [==============================] - 42s 162ms/step - loss: 0.0946 - acc: 0.6726 - val_loss: 0.3593 - val_acc: 0.5485\n",
      "Epoch 17/25\n",
      "260/260 [==============================] - 42s 162ms/step - loss: 0.0906 - acc: 0.6842 - val_loss: 0.3618 - val_acc: 0.5493\n",
      "Epoch 18/25\n",
      "260/260 [==============================] - 42s 162ms/step - loss: 0.0867 - acc: 0.6960 - val_loss: 0.3641 - val_acc: 0.5480\n",
      "Epoch 19/25\n",
      "260/260 [==============================] - 42s 163ms/step - loss: 0.0830 - acc: 0.7069 - val_loss: 0.3678 - val_acc: 0.5454\n",
      "Epoch 20/25\n",
      "260/260 [==============================] - 42s 162ms/step - loss: 0.0794 - acc: 0.7178 - val_loss: 0.3705 - val_acc: 0.5471\n",
      "Epoch 21/25\n",
      "260/260 [==============================] - 42s 161ms/step - loss: 0.0759 - acc: 0.7294 - val_loss: 0.3740 - val_acc: 0.5469\n",
      "Epoch 22/25\n",
      "260/260 [==============================] - 42s 162ms/step - loss: 0.0726 - acc: 0.7401 - val_loss: 0.3784 - val_acc: 0.5430\n",
      "Epoch 23/25\n",
      "260/260 [==============================] - 42s 162ms/step - loss: 0.0694 - acc: 0.7505 - val_loss: 0.3831 - val_acc: 0.5430\n",
      "Epoch 24/25\n",
      "260/260 [==============================] - 43s 166ms/step - loss: 0.0663 - acc: 0.7610 - val_loss: 0.3873 - val_acc: 0.5415\n",
      "Epoch 25/25\n",
      "260/260 [==============================] - 43s 167ms/step - loss: 0.0634 - acc: 0.7708 - val_loss: 0.3923 - val_acc: 0.5408\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f5c2093ca0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_input_data_padded,decoder_input_data_padded], decoder_target_data_padedd,\n",
    "          batch_size=bat_size,\n",
    "          epochs=25,\n",
    "          validation_data=([encoder_input_data_val_padded, decoder_input_data_val_padded], decoder_target_data_val_padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_embedding_1024.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
